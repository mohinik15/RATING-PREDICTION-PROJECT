{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0063cde",
   "metadata": {},
   "source": [
    "# RATING PREDICTION PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c205a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Importing nltk libraries\n",
    "import re\n",
    "import string\n",
    "import missingno\n",
    "import pandas_profiling\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import FreqDist\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "from scipy.sparse import hstack\n",
    "import scikitplot as skplt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score, precision_score, confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import joblib\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc9300",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Review_Rating_Datafile.csv\")\n",
    "df # checking the first five and last five rows of our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ccda38",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e6147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = 'Unnamed: 0', inplace = True)\n",
    "print(\"We have {} Rows and {} Columns in our dataframe\".format(df.shape[0], df.shape[1]))\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf30ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() # checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c1879",
   "metadata": {},
   "outputs": [],
   "source": [
    "missingno.matrix(df, figsize = (15,8), color=(0.25, 0.75, 0.25), fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f26e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have {} Rows and {} Columns in our dataframe before removing NaN\".format(df.shape[0], df.shape[1]))\n",
    "df.dropna(inplace=True)\n",
    "print(\"We have {} Rows and {} Columns in our dataframe after removing NaN\".format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913dc401",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64690771",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04eeecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ratings'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b19db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ratings'] = df['Ratings'].replace('1.0 out of 5 stars',1)\n",
    "df['Ratings'] = df['Ratings'].replace('2.0 out of 5 stars',2)\n",
    "df['Ratings'] = df['Ratings'].replace('3.0 out of 5 stars',3)\n",
    "df['Ratings'] = df['Ratings'].replace('4.0 out of 5 stars',4)\n",
    "df['Ratings'] = df['Ratings'].replace('5.0 out of 5 stars',5)\n",
    "df['Ratings'] = df['Ratings'].astype('int')\n",
    "df['Ratings'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d133de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now combining the \"Review_title\" and \"Review_text\" columns into one single column called \"Review\"\n",
    "df['Review'] = df['Review_title'].map(str)+' '+df['Review_text']\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "acfc3599",
   "metadata": {},
   "source": [
    "Visualizing text in first three rows from the newly created \"Review\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b48b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fbe8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a65da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][2]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9c4d0e0",
   "metadata": {},
   "source": [
    "Text Processing to remove unwanted punctuations and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888395c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Here I am defining a function to replace some of the contracted words to their full form and removing urls and some \n",
    "unwanted text'''\n",
    "\n",
    "def decontracted(text):\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"don’t\", \"do not\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"im \", \"i am\", text)\n",
    "    text = re.sub(r\"yo \", \"you \",text)\n",
    "    text = re.sub(r\"doesn’t\", \"does not\",text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"<br>\", \" \", text)\n",
    "    text = re.sub(r'http\\S+', '', text) #removing urls\n",
    "    return text\n",
    "\n",
    "# Lowercasing the alphabets\n",
    "df['Review'] = df['Review'].apply(lambda x : x.lower())\n",
    "df['Review'] = df['Review'].apply(lambda x : decontracted(x))\n",
    "\n",
    "# Removing punctuations from the review\n",
    "df['Review'] = df['Review'].str.replace('[^\\w\\s]','')\n",
    "df['Review'] = df['Review'].str.replace('\\n',' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a914e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all the stopwords\n",
    "stop = stopwords.words('english')\n",
    "df['Review'] = df['Review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4691a537",
   "metadata": {},
   "source": [
    "# Visualizing text in first three rows after applying various text cleaning procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b17599",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5264d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec5c824",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57c4d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functiom to convert nltk tag to wordnet tags\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# Defining function to lemmatize our text\n",
    "def lemmatize_sentence(sentence):\n",
    "    # tokenize the sentence and find the pos_tag\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    # tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x : (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatize_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatize_sentence.append(word)\n",
    "        else:\n",
    "            lemmatize_sentence.append(lemmatizer.lemmatize(word,tag))\n",
    "    return \" \".join(lemmatize_sentence)    \n",
    "\n",
    "df['Review'] = df['Review'].apply(lambda x : lemmatize_sentence(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e19b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab88588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccfdb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8efec08",
   "metadata": {},
   "source": [
    "# Text Normalization - Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1fbaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise removal function\n",
    "def scrub_words(text):\n",
    "    # remove HTML markup\n",
    "    text = re.sub(\"(<.*?>)\", \"\", text)\n",
    "    # remove non-ascii and digits\n",
    "    text = re.sub(\"(\\\\W)\", \" \", text)\n",
    "    text = re.sub(\"(\\\\d)\", \"\", text)\n",
    "    # remove white space\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "df['Review'] = df['Review'].apply(lambda x : scrub_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c862a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e01f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc0fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d90b1",
   "metadata": {},
   "source": [
    "# WORD COUNTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9081b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating column for word counts in the review text\n",
    "df['Review_WC'] = df['Review'].apply(lambda x: len(str(x).split(' ')))\n",
    "df[['Review_WC', 'Review']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5954dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot and histogram of all word count\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.distplot(df['Review_WC'], hist = True, kde = True,\n",
    "            bins = int(180/5), color = 'darkblue',\n",
    "            hist_kws = {'edgecolor':'black'},\n",
    "            kde_kws = {'linewidth':4})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f104abab",
   "metadata": {},
   "source": [
    "# CHARACTER COUNTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654df3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating column for character counts in the review text\n",
    "df['Review_CC'] = df['Review'].str.len()\n",
    "df[['Review_CC','Review']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c6c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot and histogram of all character count\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.distplot(df['Review_CC'], hist = True, kde = True,\n",
    "            bins = int(180/5), color = 'darkblue',\n",
    "            hist_kws = {'edgecolor':'black'},\n",
    "            kde_kws = {'linewidth':4})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9005130",
   "metadata": {},
   "source": [
    "# REMOVING OUTLIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1986c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying zscore to remove outliers\n",
    "z_score = zscore(df[['Review_WC']])\n",
    "abs_z_score = np.abs(z_score)\n",
    "filtering_entry = (abs_z_score < 3).all(axis = 1)\n",
    "df = df[filtering_entry]\n",
    "print(\"We have {} Rows and {} Columns in our dataframe after removing outliers\".format(df.shape[0], df.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d65df",
   "metadata": {},
   "source": [
    "# VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ab79a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c59ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot and histogram of all word count\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.distplot(df['Review_WC'], hist = True, kde = True,\n",
    "            bins = int(180/5), color = 'darkblue',\n",
    "            hist_kws = {'edgecolor':'black'},\n",
    "            kde_kws = {'linewidth':4})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e713f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot and histogram of all character count\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.distplot(df['Review_CC'], hist = True, kde = True,\n",
    "            bins = int(180/5), color = 'darkblue',\n",
    "            hist_kws = {'edgecolor':'black'},\n",
    "            kde_kws = {'linewidth':4})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3121e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the ratings column details using count plot\n",
    "x = 'Ratings'\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,8))\n",
    "sns.countplot(x=x,data=df,ax=ax)\n",
    "p=0\n",
    "for i in ax.patches:\n",
    "    q = i.get_height()/2\n",
    "    val = i.get_height()\n",
    "    ratio = round(val*100/len(df),2)\n",
    "    prn = f\"{val}\\n({ratio}%)\"\n",
    "    ax.text(p,q,prn,ha=\"center\",color=\"white\",rotation=0,fontweight=\"bold\",fontsize=\"13\")\n",
    "    p += 1\n",
    "    \n",
    "plt.title(\"Count Plot: rating\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8da583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking review word count distribution for each rating\n",
    "ratings = np.sort(df.Ratings.unique())\n",
    "cols = 3\n",
    "rows = len(ratings)//cols\n",
    "if rows % cols != 0:\n",
    "    rows += 1\n",
    "    \n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.2)\n",
    "p = 1\n",
    "colors = [(1,0,0,1),(0.6,0.2,0,1),(0.4,0.5,0,1),(0.2,0.7,0,1),(0,1,0.1,1)]\n",
    "for i in ratings:\n",
    "    axis = fig.add_subplot(rows,cols,p)\n",
    "    sns.distplot(df.Review_WC[df.Ratings==i], ax=axis, label=f\"For Rating: {i}\", color=colors[i-1])\n",
    "    axis.set_xlabel(f\"Review Word Count\")\n",
    "    axis.legend()\n",
    "    p += 1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f0388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking review character count distribution for each rating\n",
    "ratings = np.sort(df.Ratings.unique())\n",
    "cols = 3\n",
    "rows = len(ratings)//cols\n",
    "if rows % cols != 0:\n",
    "    rows += 1\n",
    "    \n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.2)\n",
    "p = 1\n",
    "colors = [(1,0,0,1),(0.6,0.2,0,1),(0.4,0.5,0,1),(0.2,0.7,0,1),(0,1,0.1,1)]\n",
    "for i in ratings:\n",
    "    axis = fig.add_subplot(rows,cols,p)\n",
    "    sns.distplot(df.Review_CC[df.Ratings==i], ax=axis, label=f\"For Rating: {i}\", color=colors[i-1])\n",
    "    axis.set_xlabel(f\"Review Character Count\")\n",
    "    axis.legend()\n",
    "    p += 1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280df334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting insight of loud words in each rating\n",
    "cols = 2\n",
    "ratings = np.sort(df.Ratings.unique())\n",
    "rows = len(ratings)//2\n",
    "if len(ratings) % cols != 0:\n",
    "    rows += 1    \n",
    "fig = plt.figure(figsize=(15,20))\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "p = 1\n",
    "for i in ratings:\n",
    "    word_cloud = WordCloud(height=800, width=1000, background_color=\"white\", max_words=50).generate(' '.join(df.Review[df.Ratings==i]))\n",
    "    axis = fig.add_subplot(rows,cols,p)\n",
    "    axis.set_title(f\"WordCloud for Rating: {i}\\n\")\n",
    "    axis.imshow(word_cloud)\n",
    "    for spine in axis.spines.values():\n",
    "        spine.set_edgecolor('r')\n",
    "    axis.set_xticks([])\n",
    "    axis.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout(pad=5)\n",
    "    p += 1    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a99391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the count of target column values\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.countplot(df['Ratings'])\n",
    "print(df.Ratings.value_counts())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce3a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling the entire data set\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6974f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data from every Ratings category\n",
    "df1 = df[df['Ratings']==1][0:7356]\n",
    "df2 = df[df['Ratings']==2][0:7356]\n",
    "df3 = df[df['Ratings']==3][0:7356]\n",
    "df4 = df[df['Ratings']==4][0:7356]\n",
    "df5 = df[df['Ratings']==5][0:7356]\n",
    "\n",
    "# Combining all the dataframes into one and shuffling them again\n",
    "df = pd.concat([df1,df2,df3,df4,df5], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79f4666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets Check the count of target column again to confirm the balance with a visual\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.countplot(df['Ratings'])\n",
    "print(df.Ratings.value_counts())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e456143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot most frequent terms in our Review column\n",
    "def freq_words(x, terms=30):\n",
    "    all_words = ' '.join([text for text in x])\n",
    "    all_words = all_words.split()\n",
    "    fdist = FreqDist(all_words)\n",
    "    words_df = pd.DataFrame({'word':list(fdist.keys()),\n",
    "                             'count':list(fdist.values())})\n",
    "    # selecting top 30 most frequent words\n",
    "    dt = words_df.nlargest(columns='count', n=terms)\n",
    "    plt.figure(figsize=(15,10))\n",
    "    ax = sns.barplot(data=dt, x='count', y='word')\n",
    "    ax.set(ylabel = 'Word')\n",
    "    plt.show()\n",
    "    \n",
    "freq_words(df['Review'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405a3e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot least frequent terms in our Review column\n",
    "def rare_words(x, terms=30):\n",
    "    all_words = ' '.join([text for text in x])\n",
    "    all_words = all_words.split()\n",
    "    fdist = FreqDist(all_words)\n",
    "    words_df = pd.DataFrame({'word':list(fdist.keys()),\n",
    "                             'count':list(fdist.values())})\n",
    "    # selecting top 30 least freq rare words\n",
    "    dt = words_df.nsmallest(columns='count', n=terms)\n",
    "    plt.figure(figsize=(15,10))\n",
    "    ax = sns.barplot(data=dt, x='count', y='word')\n",
    "    ax.set(ylabel = 'Word')\n",
    "    plt.show()\n",
    "    \n",
    "rare_words(df['Review'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6630d59b",
   "metadata": {},
   "source": [
    "# Model Building and Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62c4453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating features and labels here\n",
    "x = df['Review']\n",
    "y = df['Ratings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad812df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the n_gram tfidf vectorizer (Word vectors)\n",
    "word_vectorizer = TfidfVectorizer(sublinear_tf = True,\n",
    "                                  strip_accents = 'unicode',\n",
    "                                  analyzer = 'word',\n",
    "                                  token_pattern = r'\\w{1,}',\n",
    "                                  stop_words = 'english',\n",
    "                                  ngram_range = (1,3),\n",
    "                                  max_features = 100000)\n",
    "word_vectorizer.fit(x)\n",
    "train_word_features = word_vectorizer.transform(x)\n",
    "\n",
    "# Character vectors\n",
    "char_vectorizer = TfidfVectorizer(sublinear_tf = True,\n",
    "                                  strip_accents = 'unicode',\n",
    "                                  analyzer = 'char',\n",
    "                                  stop_words = 'english',\n",
    "                                  ngram_range = (2,6),\n",
    "                                  max_features = 50000)\n",
    "char_vectorizer.fit(x)\n",
    "train_char_features = char_vectorizer.transform(x)\n",
    "\n",
    "# I will now combine both word vectors and character vectors as input for our model\n",
    "train_features = hstack([train_char_features, train_word_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e43b8ed",
   "metadata": {},
   "source": [
    "# Splitting the data into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17afb093",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 42\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_features, y, test_size = 0.30, random_state = state)\n",
    "\n",
    "# Lets check the shapes of traning and test data\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"y_test\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80072d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Classification Machine Learning Algorithms\n",
    "rf = RandomForestClassifier()\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "svc = LinearSVC()\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "sgd = SGDClassifier()\n",
    "lgb = LGBMClassifier()\n",
    "xgb = XGBClassifier(verbosity=0)\n",
    "\n",
    "# Creating a function to train and test the model with evaluation metrics\n",
    "def BuiltModel(model):\n",
    "    print('*'*30+model.__class__.__name__+'*'*30)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_train)\n",
    "    pred = model.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, pred)*100\n",
    "    print(f\"ACCURACY SCORE PERCENTAGE:\", accuracy)\n",
    "    # Confusion matrix and Classification report\n",
    "    print(f\"CLASSIFICATION REPORT: \\n {classification_report(y_test, pred)}\")\n",
    "    print(f\"CONFUSION MATRIX: \\n {confusion_matrix(y_test, pred)}\\n\")\n",
    "    print(\"-\"*120)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03989069",
   "metadata": {},
   "source": [
    "# Training and testing of all the classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e1a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [lr,svc,bnb,mnb,sgd,rf,xgb,lgb]:\n",
    "    BuiltModel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a30042",
   "metadata": {},
   "source": [
    "# Cross validation score for best score models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd42334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am checking cross-validation score only for those algorithms which are giving us better accuracies\n",
    "\n",
    "def cross_val(model):\n",
    "    print('*'*30+model.__class__.__name__+'*'*30)\n",
    "    scores = cross_val_score(model,train_features,y, cv = 3).mean()*100\n",
    "    print(\"Cross validation score:\", scores)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "for model in [lr,svc,sgd,rf,lgb,xgb]:\n",
    "    cross_val(model)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e0f59",
   "metadata": {},
   "source": [
    "# HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d094d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets select the different parameters for tuning our best model (RandomForestClassifier)\n",
    "grid_params = {'n_estimators':[100,200],\n",
    "               'criterion':['gini','entropy'],\n",
    "               'max_depth': [500,800],\n",
    "               'bootstrap':[True,False]}\n",
    "\n",
    "# Train the model with given parameters using GridSearchCV\n",
    "GSCV =  GridSearchCV(rf, grid_params, cv=3, verbose=3)\n",
    "GSCV.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c948d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GSCV.best_params_ # Selecting the best parameters found by GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f2f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model with the best chosen parameters list\n",
    "best_model = RandomForestClassifier(bootstrap=False, criterion=\"gini\", max_depth=800, n_estimators=200)\n",
    "best_model.fit(x_train,y_train) # fitting data to the best model\n",
    "pred = best_model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, pred)*100\n",
    "# Printing the accuracy score\n",
    "print(\"ACCURACY SCORE:\", accuracy)\n",
    "# Printing the classification report\n",
    "print(f\"\\nCLASSIFICATION REPORT: \\n {classification_report(y_test, pred)}\")\n",
    "# Printing the Confusion matrix\n",
    "print(f\"\\nCONFUSION MATRIX: \\n {confusion_matrix(y_test, pred)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
